"""
Claude API Tokenä½¿ç”¨é‡ç»Ÿè®¡åˆ†æå™¨
è§£ææ—¥å¿—æ–‡ä»¶ï¼Œæä¾›è¯¦ç»†çš„tokenä½¿ç”¨ç»Ÿè®¡
"""

import os
import json
import re
from datetime import datetime, timedelta
from collections import defaultdict
from typing import Dict, List, Any
import glob
import threading
import tempfile

class TokenStatsAnalyzer:
    """Tokenç»Ÿè®¡åˆ†æå™¨"""

    def __init__(self, log_dir: str = "logs"):
        """åˆå§‹åŒ–åˆ†æå™¨

        Args:
            log_dir: æ—¥å¿—æ–‡ä»¶ç›®å½•
        """
        self.log_dir = log_dir
        self.stats_data = []  # å­˜å‚¨æ‰€æœ‰è§£æçš„ç»Ÿè®¡æ•°æ®

    def parse_log_files(self):
        """è§£ææ‰€æœ‰logæ–‡ä»¶"""
        print(f"å¼€å§‹è§£ææ—¥å¿—æ–‡ä»¶ç›®å½•: {self.log_dir}")

        # è·å–æ‰€æœ‰.logæ–‡ä»¶
        log_files = glob.glob(os.path.join(self.log_dir, "*.log"))
        print(f"æ‰¾åˆ° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")

        for log_file in log_files:
            print(f"æ­£åœ¨è§£æ: {log_file}")
            self._parse_single_file(log_file)

        print(f"è§£æå®Œæˆï¼å…±æå– {len(self.stats_data)} æ¡è®°å½•")

    def _parse_single_file(self, file_path: str):
        """è§£æå•ä¸ªæ—¥å¿—æ–‡ä»¶

        Args:
            file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        """
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                current_timestamp = None

                for line in f:
                    # æå–æ—¶é—´æˆ³
                    timestamp_match = re.match(r'^(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', line)
                    if timestamp_match:
                        current_timestamp = timestamp_match.group(1)

                    # æŸ¥æ‰¾åŒ…å«usageä¿¡æ¯çš„è¡Œ
                    if '"usage":' in line and '"message_start"' in line:
                        # æå–usage JSONæ•°æ®
                        try:
                            # æŸ¥æ‰¾data:åçš„JSON
                            data_match = re.search(r'data:\s*(\{.+\})', line)
                            if data_match:
                                data_json = json.loads(data_match.group(1))

                                # æå–messageä¸­çš„usage
                                if 'message' in data_json:
                                    message = data_json['message']
                                    usage = message.get('usage', {})
                                    model = message.get('model', 'unknown')

                                    # ä¿å­˜ç»Ÿè®¡æ•°æ®
                                    self.stats_data.append({
                                        'timestamp': current_timestamp or datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                        'model': model,
                                        'input_tokens': usage.get('input_tokens', 0),
                                        'cache_creation_input_tokens': usage.get('cache_creation_input_tokens', 0),
                                        'cache_read_input_tokens': usage.get('cache_read_input_tokens', 0),
                                        'output_tokens': usage.get('output_tokens', 0),
                                        'service_tier': usage.get('service_tier', 'standard')
                                    })
                        except json.JSONDecodeError:
                            continue

                    # ä¹Ÿå¤„ç†message_deltaä¸­çš„final usage
                    elif '"usage":' in line and '"message_delta"' in line:
                        try:
                            data_match = re.search(r'data:\s*(\{.+\})', line)
                            if data_match:
                                data_json = json.loads(data_match.group(1))
                                usage = data_json.get('usage', {})

                                # æ›´æ–°æœ€åä¸€æ¡è®°å½•çš„output_tokens
                                if self.stats_data and usage.get('output_tokens'):
                                    # åªæ›´æ–°output_tokensï¼ˆè¿™æ˜¯æœ€ç»ˆå€¼ï¼‰
                                    self.stats_data[-1]['output_tokens'] = usage.get('output_tokens', 0)
                        except json.JSONDecodeError:
                            continue

        except Exception as e:
            print(f"è§£ææ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")

    def get_stats_by_model(self) -> Dict[str, Any]:
        """æŒ‰æ¨¡å‹ç»Ÿè®¡tokenä½¿ç”¨é‡"""
        model_stats = defaultdict(lambda: {
            'total_input_tokens': 0,
            'total_cache_creation_tokens': 0,
            'total_cache_read_tokens': 0,
            'total_output_tokens': 0,
            'total_requests': 0,
            'total_tokens': 0  # æ€»è®¡æ‰€æœ‰token
        })

        for record in self.stats_data:
            model = record['model']
            model_stats[model]['total_input_tokens'] += record['input_tokens']
            model_stats[model]['total_cache_creation_tokens'] += record['cache_creation_input_tokens']
            model_stats[model]['total_cache_read_tokens'] += record['cache_read_input_tokens']
            model_stats[model]['total_output_tokens'] += record['output_tokens']
            model_stats[model]['total_requests'] += 1

            # è®¡ç®—æ€»tokenæ•°
            total = (record['input_tokens'] +
                    record['cache_creation_input_tokens'] +
                    record['cache_read_input_tokens'] +
                    record['output_tokens'])
            model_stats[model]['total_tokens'] += total

        return dict(model_stats)

    def get_stats_by_date(self, period: str = 'daily') -> Dict[str, Any]:
        """æŒ‰æ—¥æœŸç»Ÿè®¡tokenä½¿ç”¨é‡

        Args:
            period: ç»Ÿè®¡å‘¨æœŸ ('daily', 'weekly', 'monthly')
        """
        date_stats = defaultdict(lambda: {
            'total_input_tokens': 0,
            'total_cache_creation_tokens': 0,
            'total_cache_read_tokens': 0,
            'total_output_tokens': 0,
            'total_requests': 0,
            'total_tokens': 0,
            'models': defaultdict(int)  # å„æ¨¡å‹çš„ä½¿ç”¨æ¬¡æ•°
        })

        for record in self.stats_data:
            try:
                timestamp = datetime.strptime(record['timestamp'], '%Y-%m-%d %H:%M:%S')

                # æ ¹æ®periodç¡®å®šæ—¥æœŸkey
                if period == 'daily':
                    date_key = timestamp.strftime('%Y-%m-%d')
                elif period == 'weekly':
                    # å‘¨ä¸€ä½œä¸ºä¸€å‘¨çš„å¼€å§‹
                    week_start = timestamp - timedelta(days=timestamp.weekday())
                    date_key = week_start.strftime('%Y-W%V')
                elif period == 'monthly':
                    date_key = timestamp.strftime('%Y-%m')
                else:
                    date_key = timestamp.strftime('%Y-%m-%d')

                # ç´¯åŠ ç»Ÿè®¡
                date_stats[date_key]['total_input_tokens'] += record['input_tokens']
                date_stats[date_key]['total_cache_creation_tokens'] += record['cache_creation_input_tokens']
                date_stats[date_key]['total_cache_read_tokens'] += record['cache_read_input_tokens']
                date_stats[date_key]['total_output_tokens'] += record['output_tokens']
                date_stats[date_key]['total_requests'] += 1

                total = (record['input_tokens'] +
                        record['cache_creation_input_tokens'] +
                        record['cache_read_input_tokens'] +
                        record['output_tokens'])
                date_stats[date_key]['total_tokens'] += total
                date_stats[date_key]['models'][record['model']] += 1

            except ValueError:
                continue

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        result = {}
        for date_key, stats in date_stats.items():
            stats['models'] = dict(stats['models'])
            result[date_key] = stats

        return result

    def get_summary(self) -> Dict[str, Any]:
        """è·å–æ€»ä½“ç»Ÿè®¡æ‘˜è¦"""
        total_stats = {
            'total_input_tokens': 0,
            'total_cache_creation_tokens': 0,
            'total_cache_read_tokens': 0,
            'total_output_tokens': 0,
            'total_requests': len(self.stats_data),
            'total_tokens': 0,
            'unique_models': set(),
            'date_range': {
                'start': None,
                'end': None
            }
        }

        timestamps = []

        for record in self.stats_data:
            total_stats['total_input_tokens'] += record['input_tokens']
            total_stats['total_cache_creation_tokens'] += record['cache_creation_input_tokens']
            total_stats['total_cache_read_tokens'] += record['cache_read_input_tokens']
            total_stats['total_output_tokens'] += record['output_tokens']
            total_stats['unique_models'].add(record['model'])

            total = (record['input_tokens'] +
                    record['cache_creation_input_tokens'] +
                    record['cache_read_input_tokens'] +
                    record['output_tokens'])
            total_stats['total_tokens'] += total

            try:
                timestamp = datetime.strptime(record['timestamp'], '%Y-%m-%d %H:%M:%S')
                timestamps.append(timestamp)
            except ValueError:
                continue

        # ç¡®å®šæ—¥æœŸèŒƒå›´
        if timestamps:
            total_stats['date_range']['start'] = min(timestamps).strftime('%Y-%m-%d %H:%M:%S')
            total_stats['date_range']['end'] = max(timestamps).strftime('%Y-%m-%d %H:%M:%S')

        # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
        total_stats['unique_models'] = list(total_stats['unique_models'])

        return total_stats

    def _merge_stats(self, old_stats: Dict[str, Any], new_stats: Dict[str, Any]) -> Dict[str, Any]:
        """åˆå¹¶æ–°æ—§ç»Ÿè®¡æ•°æ®ï¼ˆç´¯è®¡æ¨¡å¼ï¼‰

        Args:
            old_stats: æ—§çš„ç»Ÿè®¡æ•°æ®
            new_stats: æ–°çš„ç»Ÿè®¡æ•°æ®

        Returns:
            åˆå¹¶åçš„ç»Ÿè®¡æ•°æ®
        """
        merged = new_stats.copy()

        # 1. åˆå¹¶ summary
        if 'summary' in old_stats and 'summary' in new_stats:
            old_summary = old_stats['summary']
            new_summary = new_stats['summary']

            merged['summary'] = {
                'total_input_tokens': old_summary.get('total_input_tokens', 0) + new_summary.get('total_input_tokens', 0),
                'total_cache_creation_tokens': old_summary.get('total_cache_creation_tokens', 0) + new_summary.get('total_cache_creation_tokens', 0),
                'total_cache_read_tokens': old_summary.get('total_cache_read_tokens', 0) + new_summary.get('total_cache_read_tokens', 0),
                'total_output_tokens': old_summary.get('total_output_tokens', 0) + new_summary.get('total_output_tokens', 0),
                'total_requests': old_summary.get('total_requests', 0) + new_summary.get('total_requests', 0),
                'total_tokens': old_summary.get('total_tokens', 0) + new_summary.get('total_tokens', 0),
                'unique_models': list(set(old_summary.get('unique_models', []) + new_summary.get('unique_models', []))),
                'date_range': {
                    'start': min(old_summary.get('date_range', {}).get('start', '9999-99-99 99:99:99'),
                               new_summary.get('date_range', {}).get('start', '9999-99-99 99:99:99')),
                    'end': max(old_summary.get('date_range', {}).get('end', '0000-00-00 00:00:00'),
                              new_summary.get('date_range', {}).get('end', '0000-00-00 00:00:00'))
                }
            }

        # 2. åˆå¹¶ by_model
        if 'by_model' in old_stats and 'by_model' in new_stats:
            merged['by_model'] = {}
            all_models = set(old_stats['by_model'].keys()) | set(new_stats['by_model'].keys())

            for model in all_models:
                old_model = old_stats['by_model'].get(model, {})
                new_model = new_stats['by_model'].get(model, {})

                merged['by_model'][model] = {
                    'total_input_tokens': old_model.get('total_input_tokens', 0) + new_model.get('total_input_tokens', 0),
                    'total_cache_creation_tokens': old_model.get('total_cache_creation_tokens', 0) + new_model.get('total_cache_creation_tokens', 0),
                    'total_cache_read_tokens': old_model.get('total_cache_read_tokens', 0) + new_model.get('total_cache_read_tokens', 0),
                    'total_output_tokens': old_model.get('total_output_tokens', 0) + new_model.get('total_output_tokens', 0),
                    'total_requests': old_model.get('total_requests', 0) + new_model.get('total_requests', 0),
                    'total_tokens': old_model.get('total_tokens', 0) + new_model.get('total_tokens', 0)
                }

        # 3. åˆå¹¶ daily/weekly/monthly ç»Ÿè®¡
        for period in ['daily', 'weekly', 'monthly']:
            if period in old_stats and period in new_stats:
                merged[period] = {}
                all_dates = set(old_stats[period].keys()) | set(new_stats[period].keys())

                for date_key in all_dates:
                    old_date = old_stats[period].get(date_key, {})
                    new_date = new_stats[period].get(date_key, {})

                    # åˆå¹¶modelsç»Ÿè®¡
                    old_models = old_date.get('models', {})
                    new_models = new_date.get('models', {})
                    merged_models = {}
                    all_model_names = set(old_models.keys()) | set(new_models.keys())
                    for model_name in all_model_names:
                        merged_models[model_name] = old_models.get(model_name, 0) + new_models.get(model_name, 0)

                    merged[period][date_key] = {
                        'total_input_tokens': old_date.get('total_input_tokens', 0) + new_date.get('total_input_tokens', 0),
                        'total_cache_creation_tokens': old_date.get('total_cache_creation_tokens', 0) + new_date.get('total_cache_creation_tokens', 0),
                        'total_cache_read_tokens': old_date.get('total_cache_read_tokens', 0) + new_date.get('total_cache_read_tokens', 0),
                        'total_output_tokens': old_date.get('total_output_tokens', 0) + new_date.get('total_output_tokens', 0),
                        'total_requests': old_date.get('total_requests', 0) + new_date.get('total_requests', 0),
                        'total_tokens': old_date.get('total_tokens', 0) + new_date.get('total_tokens', 0),
                        'models': merged_models
                    }

        return merged

    def export_to_json(self, output_file: str = 'json_data/token_stats.json', cumulative: bool = True):
        """å¯¼å‡ºç»Ÿè®¡ç»“æœä¸ºJSONæ–‡ä»¶

        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            cumulative: æ˜¯å¦å¯ç”¨ç´¯è®¡æ¨¡å¼ï¼ˆé»˜è®¤Trueï¼Œåˆå¹¶å†å²æ•°æ®è€Œä¸æ˜¯è¦†ç›–ï¼‰
        """
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºåŸºäºè„šæœ¬ç›®å½•çš„ç»å¯¹è·¯å¾„
        if not os.path.isabs(output_file):
            script_dir = os.path.dirname(os.path.abspath(__file__))
            output_file = os.path.join(script_dir, output_file)

        # ç”Ÿæˆå½“å‰æ—¥å¿—çš„ç»Ÿè®¡æ•°æ®
        new_stats = {
            'summary': self.get_summary(),
            'by_model': self.get_stats_by_model(),
            'daily': self.get_stats_by_date('daily'),
            'weekly': self.get_stats_by_date('weekly'),
            'monthly': self.get_stats_by_date('monthly'),
            'generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

        # å¦‚æœå¯ç”¨ç´¯è®¡æ¨¡å¼ï¼Œä¸”å­˜åœ¨æ—§çš„ç»Ÿè®¡æ–‡ä»¶ï¼Œåˆ™åˆå¹¶æ•°æ®
        if cumulative and os.path.exists(output_file):
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    old_stats = json.load(f)

                print(f"æ£€æµ‹åˆ°ç°æœ‰ç»Ÿè®¡æ–‡ä»¶ï¼Œå¯ç”¨ç´¯è®¡æ¨¡å¼...")
                print(f"  - æ—§ç»Ÿè®¡: {old_stats.get('summary', {}).get('total_requests', 0)} æ¬¡è¯·æ±‚")
                print(f"  - æ–°å¢: {new_stats.get('summary', {}).get('total_requests', 0)} æ¬¡è¯·æ±‚")

                # åˆå¹¶æ–°æ—§æ•°æ®
                stats_result = self._merge_stats(old_stats, new_stats)

                print(f"  - ç´¯è®¡: {stats_result.get('summary', {}).get('total_requests', 0)} æ¬¡è¯·æ±‚")
            except Exception as e:
                print(f"è¯»å–æ—§ç»Ÿè®¡æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†è¦†ç›–å†™å…¥")
                stats_result = new_stats
        else:
            stats_result = new_stats

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(stats_result, f, ensure_ascii=False, indent=2)

        print(f"ç»Ÿè®¡ç»“æœå·²å¯¼å‡ºåˆ°: {output_file}")
        return stats_result


# ============================================================
# TokenStatsManager - å®æ—¶ç»Ÿè®¡ç®¡ç†å™¨ï¼ˆæ–°å¢ï¼‰
# ============================================================

class TokenStatsManager:
    """Tokenç»Ÿè®¡ç®¡ç†å™¨ - çº¿ç¨‹å®‰å…¨ã€åŸå­å†™å…¥ã€å®æ—¶ç»Ÿè®¡"""

    _instance = None
    _lock = threading.Lock()

    def __new__(cls, *args, **kwargs):
        """å•ä¾‹æ¨¡å¼"""
        if not cls._instance:
            with cls._lock:
                if not cls._instance:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, stats_file="json_data/token_stats.json"):
        if hasattr(self, '_initialized'):
            return

        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºåŸºäºè„šæœ¬ç›®å½•çš„ç»å¯¹è·¯å¾„
        if not os.path.isabs(stats_file):
            script_dir = os.path.dirname(os.path.abspath(__file__))
            stats_file = os.path.join(script_dir, stats_file)

        self.stats_file = os.path.abspath(stats_file)
        self.file_lock = threading.Lock()  # æ–‡ä»¶æ“ä½œé”
        self._initialized = True

        # ç¡®ä¿æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(self.stats_file):
            self._init_empty_stats()

    def _init_empty_stats(self):
        """åˆå§‹åŒ–ç©ºçš„ç»Ÿè®¡æ–‡ä»¶"""
        empty_stats = {
            "summary": {
                "total_requests": 0,
                "total_tokens": 0,
                "total_input_tokens": 0,
                "total_output_tokens": 0,
                "total_cache_creation_tokens": 0,
                "total_cache_read_tokens": 0,
                "unique_models": []
            },
            "by_model": {},
            "daily": {},
            "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        self._atomic_write(empty_stats)

    def _atomic_write(self, data: Dict[str, Any]):
        """åŸå­å†™å…¥JSONæ–‡ä»¶ï¼ˆä¸´æ—¶æ–‡ä»¶+renameï¼‰"""
        # 0. ç¡®ä¿ç›®å½•å­˜åœ¨
        stats_dir = os.path.dirname(self.stats_file)
        if stats_dir and not os.path.exists(stats_dir):
            os.makedirs(stats_dir, exist_ok=True)

        # 1. å†™å…¥ä¸´æ—¶æ–‡ä»¶
        fd, temp_path = tempfile.mkstemp(
            dir=stats_dir,
            prefix='.tmp_stats_',
            suffix='.json'
        )

        try:
            with os.fdopen(fd, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            # 2. åŸå­é‡å‘½åï¼ˆè¿™æ˜¯åŸå­æ“ä½œï¼‰
            os.replace(temp_path, self.stats_file)
        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.remove(temp_path)
            except:
                pass
            raise e

    def _get_empty_stats_structure(self):
        """è·å–ç©ºçš„ç»Ÿè®¡æ•°æ®ç»“æ„"""
        return {
            "summary": {
                "total_requests": 0,
                "total_tokens": 0,
                "total_input_tokens": 0,
                "total_output_tokens": 0,
                "total_cache_creation_tokens": 0,
                "total_cache_read_tokens": 0,
                "unique_models": []
            },
            "by_model": {},
            "daily": {},
            "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

    def record_usage(self, model: str, usage_data: Dict[str, int], request_id: str = ""):
        """
        è®°å½•å•æ¬¡APIè°ƒç”¨çš„usageï¼ˆå®æ—¶ç»Ÿè®¡ï¼‰

        Args:
            model: æ¨¡å‹åç§°
            usage_data: usageæ•°æ®å­—å…¸ï¼Œæ”¯æŒä¸¤ç§æ ¼å¼ï¼š
                - Claudeæ ¼å¼ï¼š{input_tokens, output_tokens, cache_creation_input_tokens, cache_read_input_tokens}
                - Codexæ ¼å¼ï¼š{prompt_tokens, completion_tokens, total_tokens}
            request_id: è¯·æ±‚IDï¼ˆå¯é€‰ï¼‰
        """
        with self.file_lock:  # å¹¶å‘å®‰å…¨
            try:
                # 1. è¯»å–ç°æœ‰æ•°æ®
                if os.path.exists(self.stats_file):
                    with open(self.stats_file, 'r', encoding='utf-8') as f:
                        stats = json.load(f)
                else:
                    stats = self._get_empty_stats_structure()

                # 2. è§„èŒƒåŒ–usageæ•°æ®ï¼ˆå…¼å®¹ä¸¤ç§æ ¼å¼ï¼‰
                input_tokens = usage_data.get('input_tokens') or usage_data.get('prompt_tokens', 0)
                output_tokens = usage_data.get('output_tokens') or usage_data.get('completion_tokens', 0)
                cache_creation = usage_data.get('cache_creation_input_tokens', 0)
                cache_read = usage_data.get('cache_read_input_tokens', 0)
                total_tokens = usage_data.get('total_tokens', input_tokens + output_tokens)

                # 3. æ›´æ–°summary
                stats['summary']['total_requests'] += 1
                stats['summary']['total_tokens'] += total_tokens
                stats['summary']['total_input_tokens'] += input_tokens
                stats['summary']['total_output_tokens'] += output_tokens
                stats['summary']['total_cache_creation_tokens'] += cache_creation
                stats['summary']['total_cache_read_tokens'] += cache_read

                # æ›´æ–°unique_models
                if model not in stats['summary']['unique_models']:
                    stats['summary']['unique_models'].append(model)

                # 4. æ›´æ–°by_model
                if model not in stats['by_model']:
                    stats['by_model'][model] = {
                        'total_requests': 0,
                        'total_tokens': 0,
                        'total_input_tokens': 0,
                        'total_output_tokens': 0,
                        'total_cache_creation_tokens': 0,
                        'total_cache_read_tokens': 0
                    }

                stats['by_model'][model]['total_requests'] += 1
                stats['by_model'][model]['total_tokens'] += total_tokens
                stats['by_model'][model]['total_input_tokens'] += input_tokens
                stats['by_model'][model]['total_output_tokens'] += output_tokens
                stats['by_model'][model]['total_cache_creation_tokens'] += cache_creation
                stats['by_model'][model]['total_cache_read_tokens'] += cache_read

                # 5. æ›´æ–°daily
                today = datetime.now().strftime("%Y-%m-%d")

                if today not in stats['daily']:
                    stats['daily'][today] = {
                        'total_requests': 0,
                        'total_tokens': 0,
                        'total_input_tokens': 0,
                        'total_output_tokens': 0,
                        'total_cache_creation_tokens': 0,
                        'total_cache_read_tokens': 0,
                        'models': {}
                    }

                stats['daily'][today]['total_requests'] += 1
                stats['daily'][today]['total_tokens'] += total_tokens
                stats['daily'][today]['total_input_tokens'] += input_tokens
                stats['daily'][today]['total_output_tokens'] += output_tokens
                stats['daily'][today]['total_cache_creation_tokens'] += cache_creation
                stats['daily'][today]['total_cache_read_tokens'] += cache_read

                # æ›´æ–°dailyä¸­çš„models
                if model not in stats['daily'][today]['models']:
                    stats['daily'][today]['models'][model] = 0
                stats['daily'][today]['models'][model] += 1

                # 6. æ›´æ–°ç”Ÿæˆæ—¶é—´
                stats['generated_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                # 7. åŸå­å†™å…¥
                self._atomic_write(stats)

            except Exception as e:
                print(f"è®°å½•ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()


# å…¨å±€å•ä¾‹ï¼ˆç”¨äº app.py å¯¼å…¥ï¼‰
stats_mgr = None

def get_stats_manager(stats_file=None):
    """è·å–ç»Ÿè®¡ç®¡ç†å™¨å•ä¾‹"""
    global stats_mgr
    if stats_mgr is None:
        if stats_file is None:
            script_dir = os.path.dirname(os.path.abspath(__file__))
            stats_file = os.path.join(script_dir, "json_data", "token_stats.json")
        stats_mgr = TokenStatsManager(stats_file)
    return stats_mgr


def main():
    """ä¸»å‡½æ•° - å·²æ”¹ä¸ºå®æ—¶ç»Ÿè®¡æ¨¡å¼ï¼Œæ­¤è„šæœ¬ä¸å†ç”¨äºæ—¥å¿—è§£æ"""
    print("=" * 60)
    print("Claude API Tokenä½¿ç”¨é‡ç»Ÿè®¡åˆ†æå™¨")
    print("=" * 60)
    print()
    print("âš ï¸  æ³¨æ„ï¼šç»Ÿè®¡åŠŸèƒ½å·²æ”¹ä¸ºå®æ—¶è®°å½•æ¨¡å¼")
    print()
    print("ğŸ“Š ç»Ÿè®¡æ•°æ®ç°åœ¨é€šè¿‡ä»¥ä¸‹æ–¹å¼è‡ªåŠ¨æ›´æ–°ï¼š")
    print("   - æ¯æ¬¡APIè°ƒç”¨åè‡ªåŠ¨è®°å½•usageæ•°æ®")
    print("   - æ•°æ®å®æ—¶ä¿å­˜åˆ° token_stats.json")
    print("   - æ— éœ€æ‰‹åŠ¨æ‰§è¡Œæ­¤è„šæœ¬è¿›è¡Œç»Ÿè®¡")
    print()
    print("ğŸ’¡ å¦‚éœ€æŸ¥çœ‹å½“å‰ç»Ÿè®¡æ•°æ®ï¼š")
    print("   - è®¿é—®ç®¡ç†é¡µé¢ï¼šhttp://localhost:8080/admin")
    print("   - æˆ–ç›´æ¥æŸ¥çœ‹ï¼štoken_stats.json æ–‡ä»¶")
    print()

    # æ˜¾ç¤ºå½“å‰ç»Ÿè®¡æ–‡ä»¶çš„çŠ¶æ€
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_file = os.path.join(script_dir, "json_data", "token_stats.json")

    if os.path.exists(output_file):
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            summary = data.get('summary', {})
            print("=" * 60)
            print("å½“å‰ç»Ÿè®¡æ•°æ®æ¦‚è§ˆï¼š")
            print("=" * 60)
            print(f"  * æ€»è¯·æ±‚æ•°: {summary.get('total_requests', 0):,}")
            print(f"  * æ€»Tokenæ•°: {summary.get('total_tokens', 0):,}")
            print(f"  * æœ€åæ›´æ–°: {data.get('generated_at', 'æœªçŸ¥')}")
            print(f"  * ä½¿ç”¨æ¨¡å‹æ•°: {len(summary.get('unique_models', []))}")
        except Exception as e:
            print(f"è¯»å–ç»Ÿè®¡æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    else:
        print("âš ï¸  ç»Ÿè®¡æ–‡ä»¶å°šæœªç”Ÿæˆ")
        print("   åœ¨é¦–æ¬¡APIè°ƒç”¨åå°†è‡ªåŠ¨åˆ›å»º")

    print()
    print("=" * 60)
    print()
    print("ğŸ’¡ æç¤ºï¼šå¦‚éœ€æ¸…ç©ºç»Ÿè®¡æ•°æ®ï¼Œè¯·ä½¿ç”¨ç®¡ç†é¡µé¢çš„'æ¸…ç©ºæ•°æ®'æŒ‰é’®")


if __name__ == "__main__":
    main()
